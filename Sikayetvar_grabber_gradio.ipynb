{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFPuGGJPdVqGphiYgOGrb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batucul/calisma/blob/main/Sikayetvar_grabber_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFqvYCngl4sY"
      },
      "outputs": [],
      "source": [
        "pip install pandas openpyxl gradio pyngrok==4.1.1 lxml requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import lxml\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "\n",
        "def setup_session():\n",
        "    \"\"\"Requests oturumunu yapılandırır ve döndürür.\"\"\"\n",
        "    session = requests.Session()\n",
        "    retry = Retry(connect=5, backoff_factor=1)\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session\n",
        "\n",
        "def get_page_count(session, url):\n",
        "    \"\"\"Belirtilen URL için sayfa sayısını alır.\"\"\"\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.28 Safari/537.36'}\n",
        "    site_request = session.get(url, headers=headers).text\n",
        "    soup = BeautifulSoup(site_request, \"lxml\")\n",
        "    return int(soup.find_all(\"a\", class_=\"page\")[-1].text)\n",
        "\n",
        "def get_complaint_links(session, url, page_count):\n",
        "    \"\"\"Tüm şikayet linklerini toplar ve döndürür.\"\"\"\n",
        "    yorum_linkleri = []\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.28 Safari/537.36'}\n",
        "    for page in range(1, page_count + 1):\n",
        "        page_url = f\"{url}?page={page}\"\n",
        "        page_request = session.get(page_url, headers=headers).text\n",
        "        soup = BeautifulSoup(page_request, \"lxml\")\n",
        "        for container in soup.find_all(\"div\", class_=\"read-more-container\"):\n",
        "            read_more_div = container.find(\"div\", class_=\"read-more\")\n",
        "            if read_more_div and read_more_div.has_attr('data-url'):\n",
        "                data_url = read_more_div['data-url']\n",
        "                yorum_linkleri.append(\"https://www.sikayetvar.com\" + data_url)\n",
        "    return yorum_linkleri\n",
        "\n",
        "def scrape_complaint(session, url):\n",
        "    \"\"\"Tek bir şikayet sayfasını kazır.\"\"\"\n",
        "    review_site_request = session.get(url, headers=headers).text\n",
        "    soup = BeautifulSoup(review_site_request, \"lxml\")\n",
        "    try:\n",
        "        baslik = soup.find(\"h1\", class_=\"complaint-detail-title\").text.strip()\n",
        "        yorum = soup.find(\"div\", class_=\"complaint-detail-description\").text.strip()\n",
        "        tarih = soup.find(\"div\", class_=\"post-time\").get_text().split(\"  \")[0]\n",
        "        goruntulenme = soup.find(\"span\", class_=\"js-view-count\").get_text()\n",
        "        kullanici_adi = soup.find(\"span\", class_=\"username\").text.strip()\n",
        "        return [url, kullanici_adi, tarih, baslik, yorum, goruntulenme]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    kargo = ['tazedirekt']\n",
        "    df_columns = ['PrimaryKey', 'href', 'kullanici_adi', 'tarih', 'baslik', 'yorum', 'goruntulenme']\n",
        "    data = []\n",
        "\n",
        "    session = setup_session()\n",
        "    for firma in kargo:\n",
        "        url = f\"https://www.sikayetvar.com/{firma}\"\n",
        "        page_count = get_page_count(session, url)\n",
        "        yorum_linkleri = get_complaint_links(session, url, page_count)\n",
        "\n",
        "        for idx, link in enumerate(yorum_linkleri, 1):\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "            complaint_data = scrape_complaint(session, link)\n",
        "            if complaint_data:\n",
        "                data.append([idx] + complaint_data)\n",
        "                print(f\"Scraping progress: {idx}/{len(yorum_linkleri)}\")\n",
        "\n",
        "    df = pd.DataFrame(data, columns=df_columns)\n",
        "    excel_filename = \"scraped_data.xlsx\"\n",
        "    df.to_excel(excel_filename, index=False)\n",
        "    print(\"Scraping completed and data written to 'scraped_data.xlsx'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "tverev7nqoGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%Gradio APP\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import lxml\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import gradio as gr\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "df_columns = ['PrimaryKey', 'href', 'kullanici_adi', 'tarih', 'baslik', 'yorum', 'goruntulenme']\n",
        "\n",
        "\n",
        "def setup_session():\n",
        "    \"\"\"Requests oturumunu yapılandırır ve döndürür.\"\"\"\n",
        "    session = requests.Session()\n",
        "    retry = Retry(connect=5, backoff_factor=1)\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session\n",
        "\n",
        "def get_page_count(session, url):\n",
        "    \"\"\"Belirtilen URL için sayfa sayısını alır.\"\"\"\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.28 Safari/537.36'}\n",
        "    site_request = session.get(url, headers=headers).text\n",
        "    soup = BeautifulSoup(site_request, \"lxml\")\n",
        "    return int(soup.find_all(\"a\", class_=\"page\")[-1].text)\n",
        "\n",
        "def get_complaint_links(session, url, page_count):\n",
        "    \"\"\"Tüm şikayet linklerini toplar ve döndürür.\"\"\"\n",
        "    yorum_linkleri = []\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.28 Safari/537.36'}\n",
        "    for page in range(1, page_count + 1):\n",
        "        page_url = f\"{url}?page={page}\"\n",
        "        page_request = session.get(page_url, headers=headers).text\n",
        "        soup = BeautifulSoup(page_request, \"lxml\")\n",
        "        for container in soup.find_all(\"div\", class_=\"read-more-container\"):\n",
        "            read_more_div = container.find(\"div\", class_=\"read-more\")\n",
        "            if read_more_div and read_more_div.has_attr('data-url'):\n",
        "                data_url = read_more_div['data-url']\n",
        "                yorum_linkleri.append(\"https://www.sikayetvar.com\" + data_url)\n",
        "    return yorum_linkleri\n",
        "\n",
        "def scrape_complaint(session, url):\n",
        "    \"\"\"Tek bir şikayet sayfasını kazır.\"\"\"\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.28 Safari/537.36'}\n",
        "    review_site_request = session.get(url, headers=headers).text\n",
        "    soup = BeautifulSoup(review_site_request, \"lxml\")\n",
        "    try:\n",
        "        baslik = soup.find(\"h1\", class_=\"complaint-detail-title\").text.strip()\n",
        "        yorum = soup.find(\"div\", class_=\"complaint-detail-description\").text.strip()\n",
        "        tarih = soup.find(\"div\", class_=\"post-time\").get_text().split(\"  \")[0]\n",
        "        goruntulenme = soup.find(\"span\", class_=\"js-view-count\").get_text()\n",
        "        kullanici_adi = soup.find(\"span\", class_=\"username\").text.strip()\n",
        "        return [url, kullanici_adi, tarih, baslik, yorum, goruntulenme]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_and_save(kargo,  progress=gr.Progress()):\n",
        "    session = setup_session()\n",
        "    url = f\"https://www.sikayetvar.com/{kargo}\"\n",
        "    page_count = get_page_count(session, url)\n",
        "    yorum_linkleri = get_complaint_links(session, url, page_count)\n",
        "\n",
        "    data = []\n",
        "    for idx, link in enumerate(yorum_linkleri):\n",
        "        time.sleep(random.uniform(0, 1))\n",
        "        complaint_data = scrape_complaint(session, link)\n",
        "        if complaint_data:\n",
        "            data.append([idx] + complaint_data)\n",
        "            print(f\"Scraping progress: {idx}/{len(yorum_linkleri)}\")\n",
        "        progress((idx + 1) / len(yorum_linkleri))\n",
        "\n",
        "    df = pd.DataFrame(data, columns=df_columns)\n",
        "    excel_filename = f\"{kargo}_scraped_data.xlsx\"\n",
        "    df.to_excel(excel_filename, index=False)\n",
        "    return excel_filename\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=scrape_and_save,\n",
        "    inputs=gr.Textbox(label=\"Firma\"),\n",
        "    outputs=gr.File(),\n",
        "    title=\"Şikayetvar Verilerini Çekme\",\n",
        "    description= \"\"\"1- Şikayetlerini indirmek istediğiniz firmayı şikayetvar.com'da bulun\n",
        "2- Firmayı bulduktan sonra adres çubuğundaki firma ismini aşağıya girin\n",
        "3- Örneğin: https://www.sikayetvar.com/aras-kargo ise programa aras-kargo yazın\"\"\"\n",
        ").queue().launch(share=True, debug=True, show_error=True)"
      ],
      "metadata": {
        "id": "rKm3hQMRr5DD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}